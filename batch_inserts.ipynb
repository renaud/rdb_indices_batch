{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62351862-569c-4f34-a47d-7bca14c44971",
   "metadata": {},
   "source": [
    "# RDB lab: Batch inserts\n",
    "\n",
    "This lab intends to demonstrate to you the benefits of batch inserts.\n",
    "\n",
    "For this, we will insert into a PostgreSQL database a substantial amount of data, first using INSERTs for each new record, then using batch inserts.\n",
    "\n",
    "This lab works best if you run it in a jupyter notebook, but you can also copy-paste the code into individual scripts.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "The only new thing here is the psycopg library. Here is how to [install it](https://www.psycopg.org/psycopg3/docs/basic/install.html):\n",
    "```python\n",
    "pip install --upgrade pip       # upgrade pip to at least 20.3\n",
    "pip install \"psycopg[binary]\"\n",
    "```\n",
    "\n",
    "\n",
    "## Step 0: create a database\n",
    "\n",
    "Let's get started. First create a new database called `rdb_lab_batch_inserts` using Beekeeper Studio:\n",
    "```sql\n",
    "CREATE DATABASE rdb_lab_batch_inserts\n",
    "```\n",
    "\n",
    "## Step 1: insert 10k records, with individual INSERTs / commits\n",
    "\n",
    "Now, run the script below. It will connect to the database (you might have to change the details in `conn_info`) and insert 10'000 random records.\n",
    "\n",
    "I recommend you read the comments to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc115c03-98c3-412d-a3d5-5e069ece9e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, took 7.01s for 10,000 records\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "from time import time\n",
    "from psycopg import sql, connect\n",
    "\n",
    "# connect to postgres\n",
    "conn_info = \"dbname=rdb_lab_batch_inserts host=localhost port=5432 user=postgres password=myverysecretpassword\"\n",
    "conn = connect(conn_info)\n",
    "\n",
    "# create a new table\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS records (id serial PRIMARY KEY, num BIGINT, txt VARCHAR);\")\n",
    "    conn.commit()\n",
    "\n",
    "size_exp = 64  # the length of txt in each row, e.g. txt will be NEIDGW if size_exp is 6     \n",
    "    \n",
    "# let's create a lot of data. We start with 10'000 entries, wow!\n",
    "with conn.cursor() as cur:\n",
    "    \n",
    "    start = time()  # let's keep track of how long things take\n",
    "    for i in range(10000): \n",
    "        # we create a random int btw 1 and 10**17 and a random string with length of size_exp\n",
    "        random_num = random.randint(1, 10**17) # so as to fit in BIGINT\n",
    "        random_str = ''.join(random.choices(string.ascii_uppercase + string.digits, k=size_exp))\n",
    "        # insert it, one record at a time\n",
    "        cur.execute(sql.SQL(f\"INSERT INTO records (num, txt) VALUES (%s, %s)\"), (random_num, random_str))            \n",
    "        conn.commit()\n",
    "\n",
    "print(f\"Done, took {round(time() - start, 2)}s for 10,000 records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986106a-4b08-4b15-ac1c-415e06d2ecfa",
   "metadata": {},
   "source": [
    "## Step 2: one single commit\n",
    "\n",
    "Note how long it took to insert the 10'000 records above. We can do better! \n",
    "\n",
    "In the code above, we call `conn.commit()` after every INSERT. \n",
    "In the code below, there is a single difference: we call `conn.commit()` just once (because `conn.commit()` is **outside** the for-loop now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530dc27a-d844-4f9d-9e41-95bfab6d557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, took 1.47s for 10,000 records\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    \n",
    "    start = time()  # let's keep track of how long things take\n",
    "    for i in range(10000): \n",
    "        # we create a random int btw 1 and 10**17 and a random string with length of size_exp\n",
    "        random_num = random.randint(1, 10**17) # so as to fit in BIGINT\n",
    "        random_str = ''.join(random.choices(string.ascii_uppercase + string.digits, k=size_exp))\n",
    "        # insert it, one record at a time\n",
    "        cur.execute(sql.SQL(f\"INSERT INTO records (num, txt) VALUES (%s, %s)\"), (random_num, random_str))            \n",
    "    conn.commit()  # <---- this is called just once\n",
    "    \n",
    "print(f\"Done, took {round(time() - start, 3)}s for 10,000 records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1d657-1f8b-4aef-b888-db769b0f214d",
   "metadata": {},
   "source": [
    "That's already much faster! But we can still do better!\n",
    "\n",
    "## Step 3: Batch inserts \n",
    "\n",
    "For this, we **batch the INSERTs**. That is, instead of sending and committing an INSERT request for each new record, we will send one large INSERT command every 10'000 records.\n",
    "\n",
    "We will insert 1M records in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b8779f-d4d3-48a2-9b03-328887ec0e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 999,000 records, took 14.41s for 1'000'000 records \n"
     ]
    }
   ],
   "source": [
    "# note: if you run this code as a standalone script, you will have to copy \n",
    "#       the first part of the script above (until where size_exp is defined).\n",
    "\n",
    "\n",
    "batch_size = 1000\n",
    "nr_batches = 1000   # 10'000 * 100 = 1M records in total\n",
    "\n",
    "total_time = 0\n",
    "for batch in range(nr_batches):\n",
    "    with conn.cursor() as cursor:\n",
    "        new_rows = [(random.randint(1, 10**17), ''.join(random.choices(string.ascii_uppercase + string.digits, k=size_exp))) for i in range(1, batch_size)]\n",
    "        \n",
    "        start = time()\n",
    "        cursor.executemany('INSERT INTO records (num, txt) VALUES (%s, %s)', new_rows)\n",
    "        conn.commit()\n",
    "        total_time += time() - start\n",
    "\n",
    "print(f\"inserted {format(batch * batch_size, ',')} records, took {round(total_time, 2)}s for 1'000'000 records \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6973071-bfca-4a2a-98d3-44a819016210",
   "metadata": {},
   "source": [
    "Notice how much faster this is! In practice, you have to experiment with different `batch_size` to see which one performs best.\n",
    "\n",
    "If for some reason you have to interrupt the script above, and then try to run it again, you might get this exception:\n",
    "\n",
    "```python\n",
    "InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block\n",
    "```\n",
    "\n",
    "One way to fix it is to commit the open transaction like this.\n",
    "\n",
    "    conn.commit()\n",
    "    \n",
    "\n",
    "## Step 4: Faster with `copy`\n",
    "\n",
    "There's even a faster way with psycopg's [copy function](https://www.psycopg.org/psycopg3/docs/basic/copy.html#copy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf535d43-a5ac-4393-be7b-f051c7eb88a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, took 1.438s for 1'000'000 records\n"
     ]
    }
   ],
   "source": [
    "nr_records = 1000000\n",
    "\n",
    "records = [(random.randint(1, 10**17), ''.join(random.choices(string.ascii_uppercase + string.digits, k=size_exp))) for i in range(1, nr_records)]\n",
    "\n",
    "start=time()\n",
    "with conn.cursor() as cursor:\n",
    "    with cursor.copy(\"COPY records (num, txt) FROM STDIN\") as copy:\n",
    "        for record in records:\n",
    "            copy.write_row(record)\n",
    "print(f\"Done, took {round(time() - start, 3)}s for 1'000'000 records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d799db2-1cc2-424c-8390-4136f9d45681",
   "metadata": {},
   "source": [
    "Quite good: we just inserted a million records in a couple of seconds! I think it's the fastest we can do with psycopg.\n",
    "\n",
    "\n",
    "Once you have inserted all records, it's good practice to close the db connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ccac093-3a63-415d-bbf7-4c3321beebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad649b-02d6-4aa0-8aae-d1c940204ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
